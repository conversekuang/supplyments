

由奇异检测引入的OCSVM以及SVM相关知识的汇总



OCSVM：

​	一类支持向量机

​	https://www.zhihu.com/question/22365729

​	

​	超平面和SVM基础	

​	https://www.sohu.com/a/206572358_160850

​	https://www.sohu.com/a/211607605_160850

​	https://www.sohu.com/a/217038044_160850



​	超平面：在多维度空间下的直线。类似于一维度的点，二维度的线，三维度的面。。。

​	二维空间里面，一条直线的方程可以表示为：

​							Ax+By+C=0

​	三维空间里面，平面的方程可以表示为：

​						Ax+By+Cz+D=0

​	n维空间里面，

​						Ax1+Bx2+Cx3+Dx4+Ex5+Fx6+....+K=0

​	可以用向量表示为
$$
W^{T}X+b = 0
$$
​	都是列向量，所以系数向量带转置，最后就是内积的表达形式。

​	`W=(w1, w2,...,)`, 其中W就是法向量。

​	

​	【自己理解，在机器学习中，n个特征就是n维空间。】



**函数间隔最大化**

找到两类样本点之间离得最近的那一部分点（称之为支持向量）

```
类似于如何找到两个村子最好分割直线，就是找到两个村子最近的两个房子形成连线，做该连线的垂直线。这两个里的最近的房子也就是支持向量。
```



对于**线性分类问题**可以使用上面的线性分类支持向量机来解决，即在N维坐标空间中可以用N维的变量组合来构建一个超平面划分正例点和负例点，**如果正例点和负例点的分布无法用一个超平面去划分**就需要用到**支持向量机非线性分类器**。



线性不可分转化为线性可分：

![](https://i.loli.net/2021/03/11/VsS64yurgeqGdzH.jpg)

在二维空间中，x和o无法找到一条直线将其分开，但是将这些点映射到三维空间中就可以用一个平面去划分了，就实现了线性可分，所以，可以考虑通过对低维空间的样本点进行映射到高维空间，

**核函数** 就是实现该转换。





SVM：

https://blog.csdn.net/myarrow/article/details/51261971

经典的支持向量机算法只给出了二类分类的算法，而在数据挖掘的实际应用中，一般要解决多类的分类问题。可以通过多个二类支持向量机的组合来解决。主要有一对多组合模式、一对一组合模式和SVM决策树；再就是通过构造多个分类器的组合来解决。主要原理是克服SVM固有的缺点，结合其他算法的优势，解决多类问题的分类精度。如：与粗集理论结合，形成一种优势互补的多类问题的组合分类器。


SVM是一个二类分类器，它的目标是找到一个超平面，使用两类数据离超平面越远越好，从而对新的数据分类更准确，即使分类器更加健壮。



one-class SVM（OCSVM）

one-class SVM及单类训练SVM，因此训练时只需要一类数据，其原理是**找出高密度区域（训练数据）与低密度区域分开的边界**，然后通过这个边界来进行分类。





